{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8b4266",
   "metadata": {},
   "source": [
    "# GPT-5.2 Reasoning Effort Parameter Testing\n",
    "\n",
    "This notebook tests the `reasoning_effort` parameter with GPT-5.2 using both:\n",
    "1. **Chat Completions API** - `reasoning_effort` parameter\n",
    "2. **Responses API** - `reasoning.effort` nested parameter\n",
    "\n",
    "According to OpenAI docs:\n",
    "- GPT-5.2 supports: `none`, `low`, `medium`, `high`, `xhigh`\n",
    "- Default is `none` (minimal reasoning, lower latency)\n",
    "- `xhigh` is new in GPT-5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb7a5ff",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4edb78bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded and client initialized\n",
      "üìÖ Test Date: 2026-01-13 11:42:21\n",
      "üì° API Version: 2025-04-01-preview\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",  # Use the same API version that works\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Environment loaded and client initialized\")\n",
    "print(f\"üìÖ Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üì° API Version: 2025-04-01-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4a871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing with model: gpt-5.2\n",
      "\n",
      "‚ö†Ô∏è  If you get 404 errors, update MODEL_NAME to match your deployment\n",
      "   Common deployment names: 'gpt-5.1', 'gpt-5.2', 'gpt-51', 'gpt-52'\n",
      "   Or check Azure OpenAI Studio for your exact deployment name\n"
     ]
    }
   ],
   "source": [
    "# Configure the model to test\n",
    "# Change this to match your actual deployment name\n",
    "MODEL_NAME = \"gpt-5.2\"  # Options: \"gpt-5.2\", \"gpt-5.1\", or your deployment name\n",
    "\n",
    "print(f\"üéØ Testing with model: {MODEL_NAME}\")\n",
    "print(f\"\\n‚ö†Ô∏è  If you get 404 errors, update MODEL_NAME to match your deployment\")\n",
    "print(f\"   Common deployment names: 'gpt-5.1', 'gpt-5.2', 'gpt-51', 'gpt-52'\")\n",
    "print(f\"   Or check Azure OpenAI Studio for your exact deployment name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b8cd0",
   "metadata": {},
   "source": [
    "### Select Model to Test\n",
    "\n",
    "Update the model name based on what's available in your deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac9c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Available Model Deployments:\n",
      "================================================================================\n",
      "  ‚Ä¢ dall-e-3-3.0\n",
      "  ‚Ä¢ dall-e-2-2.0\n",
      "  ‚Ä¢ whisper-001\n",
      "  ‚Ä¢ gpt-35-turbo-0301\n",
      "  ‚Ä¢ gpt-35-turbo-0613\n",
      "  ‚Ä¢ gpt-35-turbo-1106\n",
      "  ‚Ä¢ gpt-35-turbo-0125\n",
      "  ‚Ä¢ gpt-35-turbo-instruct-0914\n",
      "  ‚Ä¢ gpt-35-turbo-16k-0613\n",
      "  ‚Ä¢ gpt-4-0125-Preview\n",
      "  ‚Ä¢ gpt-4-1106-Preview\n",
      "  ‚Ä¢ gpt-4-0314\n",
      "  ‚Ä¢ gpt-4-0613\n",
      "  ‚Ä¢ gpt-4-32k-0314\n",
      "  ‚Ä¢ gpt-4-32k-0613\n",
      "  ‚Ä¢ gpt-4-vision-preview\n",
      "  ‚Ä¢ gpt-4-turbo-2024-04-09\n",
      "  ‚Ä¢ gpt-4-turbo-jp\n",
      "  ‚Ä¢ gpt-4o-2024-05-13\n",
      "  ‚Ä¢ gpt-4o-2024-08-06\n",
      "  ‚Ä¢ gpt-4o-mini-2024-07-18\n",
      "  ‚Ä¢ gpt-4o-2024-11-20\n",
      "  ‚Ä¢ gpt-4o-audio-mai\n",
      "  ‚Ä¢ gpt-4o-realtime-preview\n",
      "  ‚Ä¢ gpt-4o-mini-realtime-preview-2024-12-17\n",
      "  ‚Ä¢ gpt-4o-realtime-preview-2024-12-17\n",
      "  ‚Ä¢ gpt-4o-realtime-preview-2025-06-03\n",
      "  ‚Ä¢ gpt-4o-canvas-2024-09-25\n",
      "  ‚Ä¢ gpt-4o-audio-preview-2024-10-01\n",
      "  ‚Ä¢ gpt-4o-audio-preview-2024-12-17\n",
      "  ‚Ä¢ gpt-4o-audio-preview-2025-06-03\n",
      "  ‚Ä¢ gpt-4o-mini-audio-preview-2024-12-17\n",
      "  ‚Ä¢ computer-use-preview-2025-02-11\n",
      "  ‚Ä¢ computer-use-preview-2025-03-11\n",
      "  ‚Ä¢ computer-use-preview-2025-04-15\n",
      "  ‚Ä¢ gpt-4o-transcribe-2025-03-20\n",
      "  ‚Ä¢ gpt-4o-mini-transcribe-2025-03-20\n",
      "  ‚Ä¢ gpt-4o-mini-tts-2025-03-20\n",
      "  ‚Ä¢ davinci\n",
      "  ‚Ä¢ text-similarity-davinci-001\n",
      "  ‚Ä¢ text-search-davinci-doc-001\n",
      "  ‚Ä¢ text-search-davinci-query-001\n",
      "  ‚Ä¢ babbage\n",
      "  ‚Ä¢ text-similarity-babbage-001\n",
      "  ‚Ä¢ text-search-babbage-doc-001\n",
      "  ‚Ä¢ text-search-babbage-query-001\n",
      "  ‚Ä¢ code-search-babbage-code-001\n",
      "  ‚Ä¢ code-search-babbage-text-001\n",
      "  ‚Ä¢ curie\n",
      "  ‚Ä¢ text-similarity-curie-001\n",
      "  ‚Ä¢ text-search-curie-doc-001\n",
      "  ‚Ä¢ text-search-curie-query-001\n",
      "  ‚Ä¢ ada\n",
      "  ‚Ä¢ text-similarity-ada-001\n",
      "  ‚Ä¢ text-search-ada-doc-001\n",
      "  ‚Ä¢ text-search-ada-query-001\n",
      "  ‚Ä¢ code-search-ada-code-001\n",
      "  ‚Ä¢ code-search-ada-text-001\n",
      "  ‚Ä¢ text-embedding-ada-002\n",
      "  ‚Ä¢ text-embedding-ada-002-2\n",
      "  ‚Ä¢ text-embedding-3-small\n",
      "  ‚Ä¢ text-embedding-3-large\n",
      "  ‚Ä¢ o1-mini-2024-09-12\n",
      "  ‚Ä¢ o1-2024-12-17\n",
      "  ‚Ä¢ o1-pro-2025-03-19\n",
      "  ‚Ä¢ o3-mini-alpha-2024-12-17\n",
      "  ‚Ä¢ o3-mini-2025-01-31\n",
      "  ‚Ä¢ o3-2025-04-16\n",
      "  ‚Ä¢ o3-pro-2025-06-10\n",
      "  ‚Ä¢ o3-deep-research-2025-06-26\n",
      "  ‚Ä¢ gpt-4.5-preview-2025-02-27\n",
      "  ‚Ä¢ model-router-2025-05-19\n",
      "  ‚Ä¢ model-router-2025-08-07\n",
      "  ‚Ä¢ model-router-2025-11-18\n",
      "  ‚Ä¢ codex-mini-2025-05-16\n",
      "  ‚Ä¢ aoai-sora-2025-02-28\n",
      "  ‚Ä¢ sora-2025-05-02\n",
      "  ‚Ä¢ sora-2-2025-10-06\n",
      "  ‚Ä¢ o4-mini-2025-04-16\n",
      "  ‚Ä¢ gpt-4.1-2025-04-14\n",
      "  ‚Ä¢ gpt-4.1-2025-04-14-text\n",
      "  ‚Ä¢ gpt-4.1-mini-2025-04-14\n",
      "  ‚Ä¢ gpt-4.1-nano-2025-04-14\n",
      "  ‚Ä¢ gpt-image-1-2025-04-15\n",
      "  ‚Ä¢ gpt-image-1-mini-2025-10-06\n",
      "  ‚Ä¢ gpt-5-2025-08-07\n",
      "  ‚Ä¢ gpt-5-mini-2025-08-07\n",
      "  ‚Ä¢ gpt-5-nano-2025-08-07\n",
      "  ‚Ä¢ gpt-5-chat-2025-08-07\n",
      "  ‚Ä¢ gpt-audio-2025-08-28\n",
      "  ‚Ä¢ gpt-realtime-2025-08-28\n",
      "  ‚Ä¢ gpt-5-chat-2025-08-15\n",
      "  ‚Ä¢ gpt-5-codex-2025-09-15\n",
      "  ‚Ä¢ gpt-5-chat-2025-10-03\n",
      "  ‚Ä¢ gpt-audio-mini-2025-10-06\n",
      "  ‚Ä¢ gpt-realtime-mini-2025-10-06\n",
      "  ‚Ä¢ gpt-5-pro-2025-10-06\n",
      "  ‚Ä¢ gpt-4o-transcribe-diarize-2025-10-15\n",
      "  ‚Ä¢ gpt-5.1-2025-11-13\n",
      "  ‚Ä¢ gpt-5.1-chat-2025-11-13\n",
      "  ‚Ä¢ gpt-5.1-codex-2025-11-13\n",
      "  ‚Ä¢ gpt-5.1-codex-mini-2025-11-13\n",
      "  ‚Ä¢ gpt-5.1-codex-max-2025-12-04\n",
      "  ‚Ä¢ gpt-5-mini-lite-2025-08-07\n",
      "  ‚Ä¢ gpt-5.2-2025-12-11\n",
      "  ‚Ä¢ gpt-5.2-chat-2025-12-11\n",
      "  ‚Ä¢ gpt-4o-mini-tts-2025-12-15\n",
      "  ‚Ä¢ gpt-realtime-mini-2025-12-15\n",
      "  ‚Ä¢ gpt-4o-mini-transcribe-2025-12-15\n",
      "  ‚Ä¢ gpt-image-1.5-2025-12-16\n",
      "  ‚Ä¢ gpt-audio-mini-2025-12-15\n",
      "  ‚Ä¢ AI21-Jamba-1.5-Large\n",
      "  ‚Ä¢ AI21-Jamba-1.5-Mini\n",
      "  ‚Ä¢ AI21-Jamba-Instruct\n",
      "  ‚Ä¢ qwen-3-32b\n",
      "  ‚Ä¢ qwen3-32b\n",
      "  ‚Ä¢ Codestral-2501-2\n",
      "  ‚Ä¢ cohere-command-a\n",
      "  ‚Ä¢ Cohere-command-r\n",
      "  ‚Ä¢ Cohere-command-r-08-2024\n",
      "  ‚Ä¢ Cohere-command-r-plus\n",
      "  ‚Ä¢ Cohere-command-r-plus-08-2024\n",
      "  ‚Ä¢ Cohere-embed-v3-english\n",
      "  ‚Ä¢ Cohere-embed-v3-multilingual\n",
      "  ‚Ä¢ DeepSeek-R1\n",
      "  ‚Ä¢ DeepSeek-R1-0528\n",
      "  ‚Ä¢ DeepSeek-V3\n",
      "  ‚Ä¢ DeepSeek-V3-0324\n",
      "  ‚Ä¢ DeepSeek-V3.1\n",
      "  ‚Ä¢ embed-v-4-0\n",
      "  ‚Ä¢ jais-30b-chat\n",
      "  ‚Ä¢ jais-30b-chat-2\n",
      "  ‚Ä¢ jais-30b-chat-3\n",
      "  ‚Ä¢ Llama-3.2-11B-Vision-Instruct\n",
      "  ‚Ä¢ Llama-3.2-11B-Vision-Instruct-2\n",
      "  ‚Ä¢ Llama-3.2-90B-Vision-Instruct\n",
      "  ‚Ä¢ Llama-3.2-90B-Vision-Instruct-2\n",
      "  ‚Ä¢ Llama-3.2-90B-Vision-Instruct-3\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct-2\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct-3\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct-4\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct-5\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct-9\n",
      "  ‚Ä¢ MAI-DS-R1\n",
      "  ‚Ä¢ Meta-Llama-3-70B-Instruct-6\n",
      "  ‚Ä¢ Meta-Llama-3-70B-Instruct-7\n",
      "  ‚Ä¢ Meta-Llama-3-70B-Instruct-8\n",
      "  ‚Ä¢ Meta-Llama-3-70B-Instruct-9\n",
      "  ‚Ä¢ Meta-Llama-3-8B-Instruct-6\n",
      "  ‚Ä¢ Meta-Llama-3-8B-Instruct-7\n",
      "  ‚Ä¢ Meta-Llama-3-8B-Instruct-8\n",
      "  ‚Ä¢ Meta-Llama-3-8B-Instruct-9\n",
      "  ‚Ä¢ Meta-Llama-3.1-405B-Instruct\n",
      "  ‚Ä¢ Meta-Llama-3.1-70B-Instruct\n",
      "  ‚Ä¢ Meta-Llama-3.1-70B-Instruct-2\n",
      "  ‚Ä¢ Meta-Llama-3.1-70B-Instruct-3\n",
      "  ‚Ä¢ Meta-Llama-3.1-70B-Instruct-4\n",
      "  ‚Ä¢ Meta-Llama-3.1-8B-Instruct\n",
      "  ‚Ä¢ Meta-Llama-3.1-8B-Instruct-2\n",
      "  ‚Ä¢ Meta-Llama-3.1-8B-Instruct-3\n",
      "  ‚Ä¢ Meta-Llama-3.1-8B-Instruct-4\n",
      "  ‚Ä¢ Meta-Llama-3.1-8B-Instruct-5\n",
      "  ‚Ä¢ Ministral-3B\n",
      "  ‚Ä¢ Mistral-large\n",
      "  ‚Ä¢ Mistral-large-2407\n",
      "  ‚Ä¢ Mistral-Large-2411-2\n",
      "  ‚Ä¢ Mistral-Nemo\n",
      "  ‚Ä¢ Mistral-small\n",
      "  ‚Ä¢ mistral-small-2503\n",
      "  ‚Ä¢ Mistral-Large-3\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct-3\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct-4\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct-5\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct-6\n",
      "  ‚Ä¢ Phi-3-medium-128k-instruct-7\n",
      "  ‚Ä¢ Phi-3-medium-4k-instruct-3\n",
      "  ‚Ä¢ Phi-3-medium-4k-instruct-4\n",
      "  ‚Ä¢ Phi-3-medium-4k-instruct-5\n",
      "  ‚Ä¢ Phi-3-medium-4k-instruct-6\n",
      "  ‚Ä¢ Phi-3-mini-128k-instruct-10\n",
      "  ‚Ä¢ Phi-3-mini-128k-instruct-11\n",
      "  ‚Ä¢ Phi-3-mini-128k-instruct-12\n",
      "  ‚Ä¢ Phi-3-mini-128k-instruct-13\n",
      "  ‚Ä¢ Phi-3-mini-4k-instruct-10\n",
      "  ‚Ä¢ Phi-3-mini-4k-instruct-11\n",
      "  ‚Ä¢ Phi-3-mini-4k-instruct-13\n",
      "  ‚Ä¢ Phi-3-mini-4k-instruct-14\n",
      "  ‚Ä¢ Phi-3-mini-4k-instruct-15\n",
      "  ‚Ä¢ Phi-3-small-128k-instruct-3\n",
      "  ‚Ä¢ Phi-3-small-128k-instruct-4\n",
      "  ‚Ä¢ Phi-3-small-128k-instruct-5\n",
      "  ‚Ä¢ Phi-3-small-8k-instruct-3\n",
      "  ‚Ä¢ Phi-3-small-8k-instruct-4\n",
      "  ‚Ä¢ Phi-3-small-8k-instruct-5\n",
      "  ‚Ä¢ Phi-3.5-mini-instruct\n",
      "  ‚Ä¢ Phi-3.5-mini-instruct-2\n",
      "  ‚Ä¢ Phi-3.5-mini-instruct-3\n",
      "  ‚Ä¢ Phi-3.5-mini-instruct-4\n",
      "  ‚Ä¢ Phi-3.5-mini-instruct-6\n",
      "  ‚Ä¢ Phi-3.5-MoE-instruct-2\n",
      "  ‚Ä¢ Phi-3.5-MoE-instruct-3\n",
      "  ‚Ä¢ Phi-3.5-MoE-instruct-4\n",
      "  ‚Ä¢ Phi-3.5-MoE-instruct-5\n",
      "  ‚Ä¢ Phi-3.5-vision-instruct\n",
      "  ‚Ä¢ Phi-3.5-vision-instruct-2\n",
      "  ‚Ä¢ Phi-4-2\n",
      "  ‚Ä¢ Phi-4-3\n",
      "  ‚Ä¢ Phi-4-4\n",
      "  ‚Ä¢ Phi-4-5\n",
      "  ‚Ä¢ Phi-4-6\n",
      "  ‚Ä¢ Phi-4-7\n",
      "  ‚Ä¢ Phi-4-mini-instruct\n",
      "  ‚Ä¢ Phi-4-multimodal-instruct\n",
      "  ‚Ä¢ Stable-Diffusion-3.5-Large\n",
      "  ‚Ä¢ Stable-Image-Core\n",
      "  ‚Ä¢ Stable-Image-Ultra\n",
      "  ‚Ä¢ Phi-4-reasoning\n",
      "  ‚Ä¢ Phi-4-mini-reasoning\n",
      "  ‚Ä¢ Llama-4-Maverick-17B-128E-Instruct-FP8\n",
      "  ‚Ä¢ Llama-4-Scout-17B-16E-Instruct\n",
      "  ‚Ä¢ mistral-medium-2505\n",
      "  ‚Ä¢ mistral-document-ai-2505\n",
      "  ‚Ä¢ grok-3\n",
      "  ‚Ä¢ grok-3-mini\n",
      "  ‚Ä¢ FLUX-1.1-pro\n",
      "  ‚Ä¢ FLUX.1-Kontext-pro\n",
      "  ‚Ä¢ grok-4-fast-reasoning\n",
      "  ‚Ä¢ grok-4-fast-non-reasoning\n",
      "  ‚Ä¢ gpt-oss-120b\n",
      "  ‚Ä¢ gpt-oss-20b-11\n",
      "  ‚Ä¢ claude-sonnet-4-5-20250929\n",
      "  ‚Ä¢ claude-haiku-4-5-20251001\n",
      "  ‚Ä¢ claude-opus-4-1-20250805\n",
      "  ‚Ä¢ claude-opus-4-5-20251101\n",
      "  ‚Ä¢ Kimi-K2-Thinking\n",
      "  ‚Ä¢ Cohere-rerank-v4.0-fast\n",
      "  ‚Ä¢ Cohere-rerank-v4.0-pro\n",
      "  ‚Ä¢ DeepSeek-V3.2\n",
      "  ‚Ä¢ DeepSeek-V3.2-Speciale\n",
      "  ‚Ä¢ FLUX.2-pro\n",
      "  ‚Ä¢ dall-e-3\n",
      "  ‚Ä¢ dall-e-2\n",
      "  ‚Ä¢ whisper\n",
      "  ‚Ä¢ gpt-35-turbo\n",
      "  ‚Ä¢ gpt-35-turbo-instruct\n",
      "  ‚Ä¢ gpt-35-turbo-16k\n",
      "  ‚Ä¢ gpt-4\n",
      "  ‚Ä¢ gpt-4-32k\n",
      "  ‚Ä¢ gpt-4o\n",
      "  ‚Ä¢ gpt-4o-mini\n",
      "  ‚Ä¢ computer-use-preview\n",
      "  ‚Ä¢ gpt-4o-transcribe\n",
      "  ‚Ä¢ text-embedding-ada-002\n",
      "  ‚Ä¢ o1-pro\n",
      "  ‚Ä¢ o3-mini-alpha\n",
      "  ‚Ä¢ o3-mini\n",
      "  ‚Ä¢ o3\n",
      "  ‚Ä¢ o3-pro\n",
      "  ‚Ä¢ gpt-4.5-preview\n",
      "  ‚Ä¢ model-router\n",
      "  ‚Ä¢ aoai-sora\n",
      "  ‚Ä¢ sora\n",
      "  ‚Ä¢ sora-2\n",
      "  ‚Ä¢ o4-mini\n",
      "  ‚Ä¢ gpt-4.1\n",
      "  ‚Ä¢ gpt-4.1-mini\n",
      "  ‚Ä¢ gpt-4.1-nano\n",
      "  ‚Ä¢ gpt-image-1\n",
      "  ‚Ä¢ gpt-image-1-mini\n",
      "  ‚Ä¢ gpt-4o-transcribe-diarize\n",
      "  ‚Ä¢ gpt-4o-mini-tts\n",
      "  ‚Ä¢ gpt-realtime-mini\n",
      "  ‚Ä¢ gpt-4o-mini-transcribe\n",
      "  ‚Ä¢ gpt-image-1.5\n",
      "  ‚Ä¢ gpt-audio-mini\n",
      "  ‚Ä¢ Llama-3.3-70B-Instruct\n",
      "\n",
      "‚úÖ Found 277 deployed model(s)\n",
      "\n",
      "üéØ GPT-5 series models found:\n",
      "   ‚Ä¢ gpt-5-2025-08-07\n",
      "   ‚Ä¢ gpt-5-mini-2025-08-07\n",
      "   ‚Ä¢ gpt-5-nano-2025-08-07\n",
      "   ‚Ä¢ gpt-5-chat-2025-08-07\n",
      "   ‚Ä¢ gpt-5-chat-2025-08-15\n",
      "   ‚Ä¢ gpt-5-codex-2025-09-15\n",
      "   ‚Ä¢ gpt-5-chat-2025-10-03\n",
      "   ‚Ä¢ gpt-5-pro-2025-10-06\n",
      "   ‚Ä¢ gpt-5.1-2025-11-13\n",
      "   ‚Ä¢ gpt-5.1-chat-2025-11-13\n",
      "   ‚Ä¢ gpt-5.1-codex-2025-11-13\n",
      "   ‚Ä¢ gpt-5.1-codex-mini-2025-11-13\n",
      "   ‚Ä¢ gpt-5.1-codex-max-2025-12-04\n",
      "   ‚Ä¢ gpt-5-mini-lite-2025-08-07\n",
      "   ‚Ä¢ gpt-5.2-2025-12-11\n",
      "   ‚Ä¢ gpt-5.2-chat-2025-12-11\n"
     ]
    }
   ],
   "source": [
    "# Check available deployments\n",
    "try:\n",
    "    # List models/deployments\n",
    "    models_response = client.models.list()\n",
    "    \n",
    "    print(\"üìã Available Model Deployments:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    available_models = []\n",
    "    for model in models_response:\n",
    "        print(f\"  ‚Ä¢ {model.id}\")\n",
    "        available_models.append(model.id)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Found {len(available_models)} deployed model(s)\")\n",
    "    \n",
    "    # Check for GPT-5.x models\n",
    "    gpt5_models = [m for m in available_models if 'gpt-5' in m.lower() or 'gpt5' in m.lower()]\n",
    "    if gpt5_models:\n",
    "        print(f\"\\nüéØ GPT-5 series models found:\")\n",
    "        for model in gpt5_models:\n",
    "            print(f\"   ‚Ä¢ {model}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  No GPT-5 series models found in your deployment\")\n",
    "        print(f\"   You may need to deploy GPT-5.1 or GPT-5.2 in Azure OpenAI Studio\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Could not list models: {str(e)}\")\n",
    "    print(\"\\n‚ÑπÔ∏è  You may need to check your Azure OpenAI resource in the portal\")\n",
    "    available_models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b02af8",
   "metadata": {},
   "source": [
    "## Check Available Model Deployments\n",
    "\n",
    "First, let's verify what models are deployed in your Azure OpenAI resource:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fea5265",
   "metadata": {},
   "source": [
    "## Test Problem\n",
    "\n",
    "We'll use a reasoning task that benefits from deeper thinking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b80484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt configured ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "TEST_PROMPT = \"\"\"\n",
    "A company has 3 data centers:\n",
    "- US East: 40% capacity, 200ms avg latency to users\n",
    "- EU West: 60% capacity, 150ms avg latency to users  \n",
    "- Asia: 30% capacity, 300ms avg latency to users\n",
    "\n",
    "Traffic pattern:\n",
    "- 50% from North America\n",
    "- 30% from Europe\n",
    "- 20% from Asia\n",
    "\n",
    "Requirements:\n",
    "- Target: <100ms latency for 95% of requests\n",
    "- Budget: $500K for improvements\n",
    "- Must maintain 99.99% uptime\n",
    "\n",
    "Provide:\n",
    "1. Root cause analysis of latency issues\n",
    "2. Recommended architecture changes\n",
    "3. Cost breakdown\n",
    "4. Implementation priority\n",
    "\"\"\"\n",
    "\n",
    "print(\"Test prompt configured ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4978c57d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Chat Completions API Testing\n",
    "\n",
    "Testing with `reasoning_effort` parameter (flat structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d8cfd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_chat_completions_api(reasoning_effort: str):\n",
    "    \"\"\"\n",
    "    Test Chat Completions API with reasoning_effort parameter.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Chat Completions API - reasoning_effort='{reasoning_effort}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert cloud infrastructure architect.\"},\n",
    "        {\"role\": \"user\", \"content\": TEST_PROMPT}\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Chat Completions API call with reasoning_effort\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,  # Use configured model name\n",
    "            messages=messages,\n",
    "            reasoning_effort=reasoning_effort,  # Flat parameter\n",
    "            max_completion_tokens=4000  # Use max_completion_tokens instead of max_tokens\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        content = response.choices[0].message.content\n",
    "        \n",
    "        result = {\n",
    "            \"API\": \"Chat Completions\",\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Reasoning Effort\": reasoning_effort,\n",
    "            \"Latency (s)\": round(latency, 2),\n",
    "            \"Prompt Tokens\": response.usage.prompt_tokens,\n",
    "            \"Completion Tokens\": response.usage.completion_tokens,\n",
    "            \"Total Tokens\": response.usage.total_tokens,\n",
    "            \"Response Length\": len(content),\n",
    "            \"Finish Reason\": response.choices[0].finish_reason,\n",
    "            \"Response Preview\": content[:300] + \"...\" if len(content) > 300 else content,\n",
    "            \"Full Response\": content\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Success!\")\n",
    "        print(f\"   Latency: {latency:.2f}s\")\n",
    "        print(f\"   Tokens: {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})\")\n",
    "        print(f\"   Response length: {len(content)} chars\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå Error: {error_msg}\")\n",
    "        \n",
    "        # Provide helpful error messages\n",
    "        if \"404\" in error_msg or \"not found\" in error_msg.lower():\n",
    "            print(f\"\\nüí° Tip: Model '{MODEL_NAME}' not found in your deployment.\")\n",
    "            print(f\"   1. Check your deployment name in Azure OpenAI Studio\")\n",
    "            print(f\"   2. Update the MODEL_NAME variable above\")\n",
    "            print(f\"   3. For GPT-5.1, the deployment might be named 'gpt-5.1' or 'gpt-51'\")\n",
    "        \n",
    "        return {\n",
    "            \"API\": \"Chat Completions\",\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Reasoning Effort\": reasoning_effort,\n",
    "            \"Error\": error_msg\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad6a98",
   "metadata": {},
   "source": [
    "### Test Chat Completions API with Different Reasoning Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03aebcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Testing Chat Completions API - reasoning_effort='none'\n",
      "================================================================================\n",
      "‚úÖ Success!\n",
      "   Latency: 94.44s\n",
      "   Tokens: 1954 (prompt: 164, completion: 1790)\n",
      "   Response length: 7496 chars\n",
      "\n",
      "================================================================================\n",
      "Testing Chat Completions API - reasoning_effort='low'\n",
      "================================================================================\n",
      "‚úÖ Success!\n",
      "   Latency: 112.92s\n",
      "   Tokens: 2281 (prompt: 164, completion: 2117)\n",
      "   Response length: 7164 chars\n",
      "\n",
      "================================================================================\n",
      "Testing Chat Completions API - reasoning_effort='medium'\n",
      "================================================================================\n",
      "‚úÖ Success!\n",
      "   Latency: 122.99s\n",
      "   Tokens: 2551 (prompt: 164, completion: 2387)\n",
      "   Response length: 6204 chars\n",
      "\n",
      "================================================================================\n",
      "Testing Chat Completions API - reasoning_effort='high'\n",
      "================================================================================\n",
      "‚úÖ Success!\n",
      "   Latency: 232.87s\n",
      "   Tokens: 4164 (prompt: 164, completion: 4000)\n",
      "   Response length: 0 chars\n",
      "\n",
      "================================================================================\n",
      "Testing Chat Completions API - reasoning_effort='xhigh'\n",
      "================================================================================\n",
      "‚úÖ Success!\n",
      "   Latency: 211.56s\n",
      "   Tokens: 4164 (prompt: 164, completion: 4000)\n",
      "   Response length: 0 chars\n",
      "\n",
      "================================================================================\n",
      "Chat Completions API testing complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test different reasoning effort levels\n",
    "reasoning_levels = [\"none\", \"low\", \"medium\", \"high\", \"xhigh\"]\n",
    "\n",
    "chat_completions_results = []\n",
    "\n",
    "for level in reasoning_levels:\n",
    "    result = test_chat_completions_api(level)\n",
    "    chat_completions_results.append(result)\n",
    "    time.sleep(1)  # Brief pause between requests\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Chat Completions API testing complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25724289",
   "metadata": {},
   "source": [
    "### View Chat Completions Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67105033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for Chat Completions results\n",
    "df_chat = pd.DataFrame(chat_completions_results)\n",
    "\n",
    "# Display summary columns\n",
    "display_cols = [\"API\", \"Model\", \"Reasoning Effort\", \"Latency (s)\", \"Total Tokens\", \"Response Length\", \"Finish Reason\"]\n",
    "available_cols = [col for col in display_cols if col in df_chat.columns]\n",
    "\n",
    "print(\"\\nüìä CHAT COMPLETIONS API RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if there were errors\n",
    "if 'Error' in df_chat.columns and df_chat['Error'].notna().any():\n",
    "    print(\"‚ö†Ô∏è  Some tests encountered errors:\")\n",
    "    display(df_chat)\n",
    "    print(\"\\nüí° See error messages above for troubleshooting tips\")\n",
    "else:\n",
    "    display(df_chat[available_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1daf018",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Responses API Testing\n",
    "\n",
    "Testing with `reasoning: { effort: \"...\" }` nested parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d09cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_responses_api(reasoning_effort: str):\n",
    "    \"\"\"\n",
    "    Test Responses API with nested reasoning.effort parameter.\n",
    "    Note: Responses API may not be available in all Azure OpenAI deployments yet.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing Responses API - reasoning.effort='{reasoning_effort}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Responses API call with nested reasoning parameter\n",
    "        response = client.responses.create(\n",
    "            model=MODEL_NAME,  # Use configured model name\n",
    "            input=TEST_PROMPT,\n",
    "            reasoning={\n",
    "                \"effort\": reasoning_effort  # Nested parameter\n",
    "            },\n",
    "            max_output_tokens=4000\n",
    "        )\n",
    "        \n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        # Extract content from response\n",
    "        content = \"\"\n",
    "        for item in response.items:\n",
    "            if hasattr(item, 'content') and item.content:\n",
    "                content += item.content\n",
    "        \n",
    "        result = {\n",
    "            \"API\": \"Responses\",\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Reasoning Effort\": reasoning_effort,\n",
    "            \"Latency (s)\": round(latency, 2),\n",
    "            \"Input Tokens\": response.usage.input_tokens if hasattr(response.usage, 'input_tokens') else \"N/A\",\n",
    "            \"Output Tokens\": response.usage.output_tokens if hasattr(response.usage, 'output_tokens') else \"N/A\",\n",
    "            \"Total Tokens\": response.usage.total_tokens if hasattr(response.usage, 'total_tokens') else \"N/A\",\n",
    "            \"Response Length\": len(content),\n",
    "            \"Response Preview\": content[:300] + \"...\" if len(content) > 300 else content,\n",
    "            \"Full Response\": content\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Success!\")\n",
    "        print(f\"   Latency: {latency:.2f}s\")\n",
    "        print(f\"   Response length: {len(content)} chars\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        error_msg = f\"Responses API not available: {str(e)}\"\n",
    "        print(f\"‚ùå {error_msg}\")\n",
    "        print(f\"\\nüí° Tip: The Responses API may not be available in Azure OpenAI yet.\")\n",
    "        print(f\"   This is a newer API that may only be in OpenAI's platform.\")\n",
    "        return {\n",
    "            \"API\": \"Responses\",\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Reasoning Effort\": reasoning_effort,\n",
    "            \"Error\": error_msg\n",
    "        }\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå Error: {error_msg}\")\n",
    "        \n",
    "        if \"404\" in error_msg or \"not found\" in error_msg.lower():\n",
    "            print(f\"\\nüí° Tip: Model '{MODEL_NAME}' not found or Responses API not supported.\")\n",
    "        \n",
    "        return {\n",
    "            \"API\": \"Responses\",\n",
    "            \"Model\": MODEL_NAME,\n",
    "            \"Reasoning Effort\": reasoning_effort,\n",
    "            \"Error\": error_msg\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6096e",
   "metadata": {},
   "source": [
    "### Test Responses API with Different Reasoning Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different reasoning effort levels\n",
    "responses_api_results = []\n",
    "\n",
    "for level in reasoning_levels:\n",
    "    result = test_responses_api(level)\n",
    "    responses_api_results.append(result)\n",
    "    time.sleep(1)  # Brief pause between requests\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Responses API testing complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161d747",
   "metadata": {},
   "source": [
    "### View Responses API Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03207c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for Responses API results\n",
    "df_responses = pd.DataFrame(responses_api_results)\n",
    "\n",
    "# Display summary columns\n",
    "display_cols = [\"API\", \"Model\", \"Reasoning Effort\", \"Latency (s)\", \"Total Tokens\", \"Response Length\"]\n",
    "available_cols = [col for col in display_cols if col in df_responses.columns]\n",
    "\n",
    "print(\"\\nüìä RESPONSES API RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check if there were errors\n",
    "if 'Error' in df_responses.columns and df_responses['Error'].notna().any():\n",
    "    print(\"‚ö†Ô∏è  Some tests encountered errors:\")\n",
    "    display(df_responses)\n",
    "    print(\"\\nüí° See error messages above for troubleshooting tips\")\n",
    "else:\n",
    "    display(df_responses[available_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08650d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Combined Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e3338",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both results\n",
    "df_combined = pd.concat([df_chat, df_responses], ignore_index=True)\n",
    "\n",
    "print(\"\\nüìä COMBINED API COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "display(df_combined[available_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b713bc8",
   "metadata": {},
   "source": [
    "### Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec078e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Only plot if we have valid data\n",
    "if 'Latency (s)' in df_combined.columns and 'Error' not in df_combined.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for api in df_combined['API'].unique():\n",
    "        data = df_combined[df_combined['API'] == api]\n",
    "        ax.plot(data['Reasoning Effort'], data['Latency (s)'], marker='o', label=api, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Reasoning Effort Level', fontsize=12)\n",
    "    ax.set_ylabel('Latency (seconds)', fontsize=12)\n",
    "    ax.set_title('Latency Comparison: Chat Completions vs Responses API', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Unable to generate latency plot - check for errors in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502bd0b",
   "metadata": {},
   "source": [
    "### Token Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Total Tokens' in df_combined.columns and 'Error' not in df_combined.columns:\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    for api in df_combined['API'].unique():\n",
    "        data = df_combined[df_combined['API'] == api]\n",
    "        if data['Total Tokens'].dtype in ['int64', 'float64']:\n",
    "            ax.plot(data['Reasoning Effort'], data['Total Tokens'], marker='s', label=api, linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Reasoning Effort Level', fontsize=12)\n",
    "    ax.set_ylabel('Total Tokens', fontsize=12)\n",
    "    ax.set_title('Token Usage Comparison: Chat Completions vs Responses API', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Unable to generate token usage plot - check for errors in results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb20cb5f",
   "metadata": {},
   "source": [
    "### View Full Response for Specific Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7782050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_full_response(df, api: str, reasoning_effort: str):\n",
    "    \"\"\"\n",
    "    Display the full response for a specific API and reasoning effort level.\n",
    "    \"\"\"\n",
    "    row = df[(df['API'] == api) & (df['Reasoning Effort'] == reasoning_effort)]\n",
    "    \n",
    "    if row.empty:\n",
    "        print(f\"‚ùå No results found for {api} with reasoning_effort='{reasoning_effort}'\")\n",
    "        return\n",
    "    \n",
    "    if 'Error' in row.columns and not pd.isna(row['Error'].values[0]):\n",
    "        print(f\"‚ùå Error occurred: {row['Error'].values[0]}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{api} API - reasoning_effort='{reasoning_effort}'\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(row['Full Response'].values[0])\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Example: View Chat Completions with \"medium\" reasoning\n",
    "# show_full_response(df_combined, \"Chat Completions\", \"medium\")\n",
    "\n",
    "# Example: View Responses API with \"high\" reasoning\n",
    "# show_full_response(df_combined, \"Responses\", \"high\")\n",
    "\n",
    "print(\"Use show_full_response(df_combined, 'API_NAME', 'reasoning_level') to view full responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c105ee87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Key Findings & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb23467",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    KEY FINDINGS & RECOMMENDATIONS                            ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "üìå API Syntax Differences:\n",
    "   ‚Ä¢ Chat Completions API: Use flat `reasoning_effort=\"medium\"` parameter\n",
    "   ‚Ä¢ Responses API: Use nested `reasoning={\"effort\": \"medium\"}` parameter\n",
    "\n",
    "üìå Reasoning Effort Levels (GPT-5.2):\n",
    "   ‚Ä¢ none   - Default, minimal reasoning, lowest latency\n",
    "   ‚Ä¢ low    - Light reasoning, faster responses\n",
    "   ‚Ä¢ medium - Balanced reasoning (recommended starting point)\n",
    "   ‚Ä¢ high   - Deep reasoning for complex problems\n",
    "   ‚Ä¢ xhigh  - NEW in GPT-5.2! Maximum reasoning capability\n",
    "\n",
    "üìå When to Use Each Level:\n",
    "   ‚Ä¢ none/low    ‚Üí Simple tasks, speed-critical applications\n",
    "   ‚Ä¢ medium      ‚Üí Most enterprise tasks, good balance\n",
    "   ‚Ä¢ high/xhigh  ‚Üí Complex reasoning, critical decisions, architecture design\n",
    "\n",
    "üìå Expected Tradeoffs:\n",
    "   ‚Ä¢ Higher reasoning effort = More latency + More tokens + Better quality\n",
    "   ‚Ä¢ Lower reasoning effort = Less latency + Fewer tokens + May need prompt tuning\n",
    "\n",
    "üìå Best Practices:\n",
    "   1. Start with 'none' and increase only if quality is insufficient\n",
    "   2. Use Responses API for multi-turn conversations (better CoT handling)\n",
    "   3. Monitor token usage - higher reasoning uses more tokens\n",
    "   4. For production: Test different levels with your specific use case\n",
    "\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ef6b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d04921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = f\"reasoning_effort_results_{timestamp}.csv\"\n",
    "\n",
    "df_combined.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úÖ Results exported to: {output_file}\")\n",
    "print(f\"üìä Total tests conducted: {len(df_combined)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
